import streamlit as st
import os
import pandas as pd
import time
import uuid
import json
import pickle
import numpy as np
from utils import save_uploaded_file
from datetime import datetime
import requests
import re
import matplotlib.pyplot as plt
import seaborn as sns
import shutil

# æ·»åŠ æ—¥å¿—å‡½æ•°
def log_info(message):
    """æ‰“å°å¸¦æ—¶é—´æˆ³çš„æ—¥å¿—ä¿¡æ¯"""
    timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
    print(f"[{timestamp}] INFO: {message}")

def log_error(message):
    """æ‰“å°å¸¦æ—¶é—´æˆ³çš„é”™è¯¯ä¿¡æ¯"""
    timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
    print(f"[{timestamp}] ERROR: {message}")

# å°è¯•å¯¼å…¥faissï¼Œå¦‚æœä¸å¯ç”¨åˆ™è®¾ç½®æ ‡å¿—
try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False

# å°è¯•å¯¼å…¥xinferenceï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨æ¨¡æ‹Ÿå®ç°
try:
    from xinference.client import RESTfulClient
    XINFERENCE_AVAILABLE = True
except ImportError:
    XINFERENCE_AVAILABLE = False
    # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„RESTfulClientç±»
    class MockRESTfulClient:
        def __init__(self, url):
            self.url = url

        def list_models(self):
            return []

        def launch_model(self, model_name, model_type):
            return {"model_id": "mock-model-id"}

    RESTfulClient = MockRESTfulClient

# å‘é‡å¤„ç†å‡½æ•°
def process_document_vectors(kb_name, document_row, client):
    """
    å¤„ç†å•ä¸ªæ–‡æ¡£çš„å‘é‡åŒ–

    å‚æ•°:
        kb_name: çŸ¥è¯†åº“åç§°
        document_row: æ–‡æ¡£å…ƒæ•°æ®è¡Œ
        client: xinferenceå®¢æˆ·ç«¯

    è¿”å›:
        å¤„ç†æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    try:
        # è·å–æ–‡ä»¶è·¯å¾„å’Œå…ƒæ•°æ®
        file_path = document_row["file_path"]
        file_type = document_row["file_type"]
        department = document_row["department"]
        document_title = document_row["title"]

        # è¯»å–æ–‡ä»¶å†…å®¹
        content = read_file_content(file_path, file_type)
        if not content:
            return False

        # ä½¿ç”¨å›ºå®šçš„æœ€ä½³å‚æ•°
        chunk_size = 300  # è¾ƒå¤§çš„åˆ†å—å¤§å°ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§
        chunk_overlap = 100  # é€‚å½“çš„é‡å ï¼Œç¡®ä¿ä¸Šä¸‹æ–‡è¿è´¯
        zh_title_enhance = True  # å¯ç”¨ä¸­æ–‡æ ‡é¢˜å¢å¼º

        # åˆ†å—å¤„ç†
        chunks = split_text_into_chunks(content, chunk_size, chunk_overlap, zh_title_enhance)

        # ä¸ºæ¯ä¸ªå—ç”Ÿæˆå‘é‡
        documents = []
        for chunk in chunks:
            # è·å–å‘é‡
            vector = get_embedding(chunk)

            # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
            doc = {
                "text": chunk,
                "vector": vector,
                "document_title": document_title,
                "department": department,
                "file_type": file_type,
                "source": file_path
            }
            documents.append(doc)

        # ä¿å­˜å‘é‡åˆ°æ–‡ä»¶
        if documents:
            vectors_dir = os.path.join("knowledge_bases", kb_name, "vectors")
            os.makedirs(vectors_dir, exist_ok=True)

            # ä½¿ç”¨UUIDä½œä¸ºæ–‡ä»¶å
            vector_file = os.path.join(vectors_dir, f"{uuid.uuid4()}.json")

            # ä¿å­˜å‘é‡æ–‡ä»¶
            with open(vector_file, "w", encoding="utf-8") as f:
                json.dump(documents, f, ensure_ascii=False, indent=2)

            log_info(f"å‘é‡æ–‡ä»¶å·²ä¿å­˜: {vector_file}ï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£")

            # æ„å»ºFAISSç´¢å¼•
            if build_faiss_index(kb_name):
                log_info(f"æˆåŠŸæ„å»ºFAISSç´¢å¼•")
            else:
                log_error(f"æ„å»ºFAISSç´¢å¼•å¤±è´¥")

        return True
    except Exception as e:
        log_error(f"å¤„ç†æ–‡æ¡£å‘é‡æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False

def split_text_into_chunks(text, chunk_size=300, chunk_overlap=100, zh_title_enhance=True):
    """
    å°†æ–‡æœ¬åˆ†å‰²æˆæ›´æœ‰è¯­ä¹‰æ„ä¹‰çš„å—

    å‚æ•°:
        text: è¦åˆ†å‰²çš„æ–‡æœ¬
        chunk_size: æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°
        chunk_overlap: å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°
        zh_title_enhance: æ˜¯å¦å¢å¼ºä¸­æ–‡æ ‡é¢˜è¯†åˆ«
    """
    if not text:
        return []

    # æŒ‰æ®µè½åˆ†å‰²
    paragraphs = re.split(r'\n{2,}', text)
    chunks = []
    current_chunk = ""

    for para in paragraphs:
        # è·³è¿‡ç©ºæ®µè½
        if not para.strip():
            continue

        # æ£€æµ‹æ˜¯å¦ä¸ºæ ‡é¢˜ï¼ˆçŸ­ä¸”ä»¥ç‰¹å®šç¬¦å·ç»“å°¾ï¼‰
        is_title = len(para.strip()) < 40 and re.search(r'[ï¼š:ã€‚ï¼Ÿï¼\?!]$', para.strip()) is None

        # å¦‚æœæ˜¯æ ‡é¢˜ä¸”å½“å‰å—ä¸ä¸ºç©ºï¼Œåˆ™å®Œæˆå½“å‰å—å¹¶å¼€å§‹æ–°å—
        if zh_title_enhance and is_title and current_chunk:
            chunks.append(current_chunk.strip())
            current_chunk = para
        # å¦‚æœæ·»åŠ æ®µè½åè¶…è¿‡å—å¤§å°ï¼Œåˆ™å®Œæˆå½“å‰å—
        elif len(current_chunk) + len(para) > chunk_size and current_chunk:
            chunks.append(current_chunk.strip())
            # æ–°å—ä»¥å‰ä¸€å—çš„ç»“å°¾å¼€å§‹ï¼ˆé‡å ï¼‰
            if chunk_overlap > 0 and len(current_chunk) > chunk_overlap:
                current_chunk = current_chunk[-chunk_overlap:] + "\n" + para
            else:
                current_chunk = para
        else:
            # æ·»åŠ æ®µè½åˆ°å½“å‰å—
            if current_chunk:
                current_chunk += "\n" + para
            else:
                current_chunk = para

    # æ·»åŠ æœ€åä¸€ä¸ªå—
    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks

def read_file_content(file_path, file_type):
    """è¯»å–ä¸åŒç±»å‹æ–‡ä»¶çš„å†…å®¹"""
    content = ""
    try:
        if file_type == "txt":
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()
        elif file_type == "pdf":
            try:
                # ä½¿ç”¨ PyMuPDF æå–PDFå†…å®¹
                import fitz  # PyMuPDF
                doc = fitz.open(file_path)
                for page in doc:
                    content += page.get_text() + "\n\n"
                doc.close()
                log_info("ä½¿ç”¨PyMuPDFæˆåŠŸæå–PDFå†…å®¹")
            except Exception as e:
                log_error(f"PDFå¤„ç†å¤±è´¥ï¼Œç¡®ä¿å·²å®‰è£…PyMuPDF: {e}")
                return ""
        elif file_type == "docx":
            try:
                import docx
                doc = docx.Document(file_path)
                for para in doc.paragraphs:
                    content += para.text + "\n"
            except ImportError:
                log_error("æœªå®‰è£…python-docxåº“ï¼Œæ— æ³•å¤„ç†DOCXæ–‡ä»¶")
                return ""
        elif file_type == "csv":
            try:
                # å°è¯•ä¸åŒçš„ç¼–ç æ–¹å¼è¯»å–CSVæ–‡ä»¶
                df = read_csv_with_encoding(file_path)
                if df is not None:
                    content = df.to_string(index=False)
            except Exception as e:
                log_error(f"å¤„ç†CSVæ–‡ä»¶æ—¶å‡ºé”™: {e}")
                return ""
        else:
            log_error(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
            return ""

        # æ–‡æœ¬æ¸…ç†å’Œè§„èŒƒåŒ–
        content = clean_text(content)
        return content
    except Exception as e:
        log_error(f"è¯»å–æ–‡ä»¶å†…å®¹æ—¶å‡ºé”™: {e}")
        return ""

def clean_text(text):
    """æ¸…ç†å’Œè§„èŒƒåŒ–æ–‡æœ¬"""
    if not text:
        return ""

    # æ›¿æ¢å¤šä¸ªç©ºæ ¼ä¸ºå•ä¸ªç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)

    # æ›¿æ¢å¤šä¸ªæ¢è¡Œä¸ºåŒæ¢è¡Œï¼ˆä¿ç•™æ®µè½ç»“æ„ï¼‰
    text = re.sub(r'\n{3,}', '\n\n', text)

    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ä½†ä¿ç•™ä¸­æ–‡æ ‡ç‚¹
    text = re.sub(r'[^\w\s\u4e00-\u9fffï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€]', '', text)

    return text.strip()

def read_csv_with_encoding(file_path):
    """å°è¯•ä¸åŒç¼–ç è¯»å–CSVæ–‡ä»¶"""
    encodings_to_try = ['utf-8', 'gbk', 'gb2312', 'gb18030', 'latin1']

    for encoding in encodings_to_try:
        try:
            df = pd.read_csv(file_path, encoding=encoding)
            log_info(f"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç è¯»å–CSVæ–‡ä»¶")
            return df
        except UnicodeDecodeError:
            continue

    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œå°è¯•æ£€æµ‹ç¼–ç 
    try:
        import chardet
        with open(file_path, 'rb') as f:
            result = chardet.detect(f.read())
        detected_encoding = result['encoding']
        log_info(f"æ£€æµ‹åˆ°CSVæ–‡ä»¶ç¼–ç : {detected_encoding}")
        return pd.read_csv(file_path, encoding=detected_encoding)
    except ImportError:
        # å¦‚æœæ²¡æœ‰å®‰è£…chardetï¼Œä½¿ç”¨latin1ä½œä¸ºåå¤‡ç¼–ç 
        return pd.read_csv(file_path, encoding='latin1')
    except Exception:
        return None

# æ·»åŠ æ„å»ºFAISSç´¢å¼•çš„å‡½æ•°
def build_faiss_index(kb_name):
    """æ„å»ºFAISSç´¢å¼•"""
    try:
        log_info(f"å¼€å§‹æ„å»ºçŸ¥è¯†åº“ '{kb_name}' çš„FAISSç´¢å¼•")

        # è·å–å‘é‡æ–‡ä»¶åˆ—è¡¨
        vectors_dir = os.path.join("knowledge_bases", kb_name, "vectors")
        if not os.path.exists(vectors_dir):
            log_error(f"å‘é‡ç›®å½•ä¸å­˜åœ¨: {vectors_dir}")
            return False

        vector_files = [f for f in os.listdir(vectors_dir) if f.endswith(".json")]
        if not vector_files:
            log_error(f"æ²¡æœ‰æ‰¾åˆ°å‘é‡æ–‡ä»¶")
            return False

        log_info(f"æ‰¾åˆ° {len(vector_files)} ä¸ªå‘é‡æ–‡ä»¶")

        # è¯»å–æ‰€æœ‰å‘é‡
        all_vectors = []
        all_metadata = []

        for file_name in vector_files:
            file_path = os.path.join(vectors_dir, file_name)
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    vectors_data = json.load(f)

                for item in vectors_data:
                    if "vector" in item and "text" in item:
                        all_vectors.append(item["vector"])

                        # å¤åˆ¶å…ƒæ•°æ®ï¼Œä½†ä¸åŒ…æ‹¬å‘é‡
                        metadata = {k: v for k, v in item.items() if k != "vector"}
                        all_metadata.append(metadata)
            except Exception as e:
                log_error(f"è¯»å–å‘é‡æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {e}")

        if not all_vectors:
            log_error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å‘é‡")
            return False

        log_info(f"è¯»å–äº† {len(all_vectors)} ä¸ªå‘é‡")

        # åˆ›å»ºFAISSç´¢å¼•
        dimension = len(all_vectors[0])
        index = faiss.IndexFlatL2(dimension)

        # å°†å‘é‡æ·»åŠ åˆ°ç´¢å¼•
        vectors_np = np.array(all_vectors).astype('float32')
        index.add(vectors_np)

        log_info(f"åˆ›å»ºäº†åŒ…å« {index.ntotal} ä¸ªå‘é‡çš„FAISSç´¢å¼•")

        # ç¡®ä¿ç´¢å¼•ç›®å½•å­˜åœ¨
        index_dir = os.path.join("knowledge_bases", kb_name, "index")
        os.makedirs(index_dir, exist_ok=True)

        # ä¿å­˜ç´¢å¼•
        index_path = os.path.join(index_dir, "faiss_index")
        faiss.write_index(index, index_path)

        # ä¿å­˜å…ƒæ•°æ®
        metadata_path = os.path.join(index_dir, "faiss_metadata.json")
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(all_metadata, f, ensure_ascii=False, indent=2)

        log_info(f"FAISSç´¢å¼•å·²ä¿å­˜åˆ°: {index_path}")
        log_info(f"å…ƒæ•°æ®å·²ä¿å­˜åˆ°: {metadata_path}")

        return True
    except Exception as e:
        log_error(f"åˆ›å»ºFAISSç´¢å¼•æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False

# æ·»åŠ æ„å»ºPickleç´¢å¼•çš„å‡½æ•°
def build_pickle_index(kb_name):
    """ä¸ºçŸ¥è¯†åº“æ„å»ºPickleç´¢å¼•"""
    vectors_dir = os.path.join("knowledge_bases", kb_name, "vectors")
    index_dir = os.path.join("knowledge_bases", kb_name, "index")
    pickle_path = os.path.join(index_dir, "vectors_index.pkl")

    if not os.path.exists(vectors_dir):
        return False

    # åˆ›å»ºç´¢å¼•ç›®å½•
    os.makedirs(index_dir, exist_ok=True)

    # æ”¶é›†æ‰€æœ‰å‘é‡æ•°æ®
    all_data = []

    # éå†æ‰€æœ‰å‘é‡æ–‡ä»¶
    for filename in os.listdir(vectors_dir):
        if filename.endswith(".json"):
            file_path = os.path.join(vectors_dir, filename)
            with open(file_path, "r", encoding="utf-8") as f:
                vectors_data = json.load(f)
                all_data.extend(vectors_data)

    if not all_data:
        return False

    # ä¿å­˜ä¸ºpickleæ–‡ä»¶
    with open(pickle_path, "wb") as f:
        pickle.dump(all_data, f)

    return True

def add_documents_to_vector_store(documents, kb_name):
    """
    å°†æ–‡æ¡£æ·»åŠ åˆ°å‘é‡åº“ï¼Œå¹¶æ£€æµ‹é‡å¤å†…å®¹

    å‚æ•°:
        documents: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£åŒ…å«textå’Œmetadata
        kb_name: çŸ¥è¯†åº“åç§°

    è¿”å›:
        æ·»åŠ çš„æ–‡æ¡£æ•°é‡
    """
    # 1. åˆ›å»ºçŸ¥è¯†åº“ç›®å½•
    kb_dir = os.path.join("knowledge_bases", kb_name)
    index_dir = os.path.join(kb_dir, "index")
    os.makedirs(index_dir, exist_ok=True)

    # 2. æ£€æŸ¥æ˜¯å¦å·²æœ‰å‘é‡åº“
    faiss_index_path = os.path.join(index_dir, "faiss_index")
    metadata_path = os.path.join(index_dir, "faiss_metadata.json")

    # 3. åŠ è½½ç°æœ‰å‘é‡åº“å’Œå…ƒæ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    existing_vectors = []
    existing_metadata = []
    existing_content_hash = set()  # ç”¨äºæ£€æµ‹é‡å¤å†…å®¹

    if os.path.exists(faiss_index_path) and os.path.exists(metadata_path):
        try:
            # åŠ è½½ç°æœ‰å‘é‡åº“
            faiss_index = faiss.read_index(faiss_index_path)
            dimension = faiss_index.d

            # æå–ç°æœ‰å‘é‡
            if faiss_index.ntotal > 0:
                existing_vectors_np = faiss.rev_swig_ptr(faiss_index.get_xb(), faiss_index.ntotal * dimension)
                existing_vectors_np = existing_vectors_np.reshape(faiss_index.ntotal, dimension)
                existing_vectors = existing_vectors_np.tolist()

            # åŠ è½½ç°æœ‰å…ƒæ•°æ®
            with open(metadata_path, "r", encoding="utf-8") as f:
                existing_metadata = json.load(f)

            # è®¡ç®—ç°æœ‰å†…å®¹çš„å“ˆå¸Œå€¼
            for doc in existing_metadata:
                text = doc.get('text', '')
                if not text and 'content' in doc:
                    text = doc.get('content', '')
                existing_content_hash.add(hash(text.strip()))

            log_info(f"åŠ è½½äº†ç°æœ‰å‘é‡åº“: {len(existing_vectors)} ä¸ªå‘é‡, {len(existing_metadata)} æ¡å…ƒæ•°æ®")
        except Exception as e:
            log_error(f"åŠ è½½ç°æœ‰å‘é‡åº“æ—¶å‡ºé”™: {e}")
            # å¦‚æœåŠ è½½å¤±è´¥ï¼Œåˆ›å»ºæ–°çš„å‘é‡åº“
            existing_vectors = []
            existing_metadata = []
            existing_content_hash = set()

    # 4. å¤„ç†æ–°æ–‡æ¡£
    new_vectors = []
    new_metadata = []
    duplicate_count = 0

    for doc in documents:
        # è·å–æ–‡æ¡£æ–‡æœ¬å’Œå…ƒæ•°æ®
        text = doc.get('text', '')
        if not text and 'content' in doc:
            text = doc.get('content', '')

        # æ£€æŸ¥æ˜¯å¦é‡å¤
        content_hash = hash(text.strip())
        if content_hash in existing_content_hash:
            duplicate_count += 1
            continue

        # è·å–æ–‡æ¡£å‘é‡
        try:
            vector = get_embedding(text)
        except Exception as e:
            log_error(f"è·å–æ–‡æ¡£å‘é‡æ—¶å‡ºé”™: {e}")
            continue

        # æ·»åŠ åˆ°æ–°å‘é‡å’Œå…ƒæ•°æ®åˆ—è¡¨
        new_vectors.append(vector)
        new_metadata.append(doc)
        existing_content_hash.add(content_hash)

    # 5. åˆ›å»ºæˆ–æ›´æ–°FAISSç´¢å¼•
    if existing_vectors or new_vectors:
        # ç¡®å®šå‘é‡ç»´åº¦
        dimension = len(new_vectors[0]) if new_vectors else len(existing_vectors[0])

        # åˆ›å»ºæ–°çš„FAISSç´¢å¼•
        faiss_index = faiss.IndexFlatL2(dimension)

        # æ·»åŠ æ‰€æœ‰å‘é‡
        all_vectors = existing_vectors + new_vectors
        if all_vectors:
            vectors_np = np.array(all_vectors).astype('float32')
            faiss_index.add(vectors_np)

        # åˆå¹¶å…ƒæ•°æ®
        all_metadata = existing_metadata + new_metadata

        # ä¿å­˜æ›´æ–°åçš„ç´¢å¼•å’Œå…ƒæ•°æ®
        faiss.write_index(faiss_index, faiss_index_path)
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(all_metadata, f, ensure_ascii=False, indent=2)

        log_info(f"å‘é‡åº“æ›´æ–°å®Œæˆ: æ·»åŠ äº† {len(new_vectors)} ä¸ªæ–‡æ¡£ï¼Œè·³è¿‡äº† {duplicate_count} ä¸ªé‡å¤æ–‡æ¡£")
        return len(new_vectors)
    else:
        log_info("æ²¡æœ‰æ–°æ–‡æ¡£æ·»åŠ åˆ°å‘é‡åº“")
        return 0

def get_embedding(text):
    """è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º"""
    url = "http://localhost:9997/v1/embeddings"

    payload = {
        "model": "bge-large-zh-v1.5",
        "input": text
    }

    headers = {
        "Content-Type": "application/json"
    }

    try:
        log_info(f"æ­£åœ¨è¯·æ±‚åµŒå…¥å‘é‡ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)}")
        start_time = time.time()
        response = requests.post(url, json=payload, headers=headers, timeout=30)
        response.raise_for_status()

        result = response.json()
        request_time = time.time() - start_time
        log_info(f"åµŒå…¥å‘é‡è¯·æ±‚å®Œæˆï¼Œè€—æ—¶: {request_time:.2f}ç§’")

        if "data" in result and len(result["data"]) > 0:
            embedding = result["data"][0]["embedding"]
            log_info(f"æˆåŠŸè·å–åµŒå…¥å‘é‡ï¼Œç»´åº¦: {len(embedding)}")
            return embedding
        else:
            log_error(f"æ— æ³•è·å–åµŒå…¥å‘é‡: {result}")
            raise Exception("æ— æ³•è·å–åµŒå…¥å‘é‡")

    except Exception as e:
        log_error(f"è·å–åµŒå…¥å‘é‡æ—¶å‡ºé”™: {e}")
        raise

def initialize_knowledge_base(kb_name):
    """åˆå§‹åŒ–çŸ¥è¯†åº“ç›®å½•ç»“æ„"""
    kb_dir = os.path.join("knowledge_bases", kb_name)
    index_dir = os.path.join(kb_dir, "index")
    vectors_dir = os.path.join(kb_dir, "vectors")

    # åˆ›å»ºç›®å½•
    os.makedirs(kb_dir, exist_ok=True)
    os.makedirs(index_dir, exist_ok=True)
    os.makedirs(vectors_dir, exist_ok=True)

    # åˆå§‹åŒ–å…ƒæ•°æ®æ–‡ä»¶
    metadata_path = os.path.join(index_dir, "faiss_metadata.json")
    if not os.path.exists(metadata_path):
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump([], f)

    log_info(f"çŸ¥è¯†åº“ '{kb_name}' åˆå§‹åŒ–å®Œæˆ")
    return True

def filter_knowledge_base(kb_name):
    """è¿‡æ»¤çŸ¥è¯†åº“ä¸­çš„ä½è´¨é‡æˆ–é‡å¤å†…å®¹"""
    log_info(f"å¼€å§‹è¿‡æ»¤çŸ¥è¯†åº“ '{kb_name}' ä¸­çš„ä½è´¨é‡æˆ–é‡å¤å†…å®¹")

    # åŠ è½½æ‰€æœ‰å‘é‡å’Œå…ƒæ•°æ®
    kb_dir = os.path.join("knowledge_bases", kb_name)
    vectors_dir = os.path.join(kb_dir, "vectors")

    if not os.path.exists(vectors_dir):
        log_error(f"å‘é‡ç›®å½•ä¸å­˜åœ¨: {vectors_dir}")
        return False

    # åˆ›å»ºå¤‡ä»½ç›®å½•
    backup_dir = os.path.join(kb_dir, "vectors_backup")
    os.makedirs(backup_dir, exist_ok=True)

    # å¤‡ä»½åŸå§‹å‘é‡æ–‡ä»¶
    backup_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    for file in os.listdir(vectors_dir):
        if file.endswith(".json"):
            src = os.path.join(vectors_dir, file)
            dst = os.path.join(backup_dir, f"{backup_time}_{file}")
            try:
                shutil.copy2(src, dst)
            except Exception as e:
                log_error(f"å¤‡ä»½æ–‡ä»¶ {file} æ—¶å‡ºé”™: {e}")

    log_info(f"å·²å¤‡ä»½åŸå§‹å‘é‡æ–‡ä»¶åˆ° {backup_dir}")

    # åŠ è½½æ‰€æœ‰æ–‡æ¡£
    all_docs = []
    vector_files = []

    for file in os.listdir(vectors_dir):
        if file.endswith(".json"):
            file_path = os.path.join(vectors_dir, file)
            vector_files.append(file_path)

            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    docs = json.load(f)
                    all_docs.extend(docs)
            except Exception as e:
                log_error(f"è¯»å–å‘é‡æ–‡ä»¶ {file} æ—¶å‡ºé”™: {e}")

    if not all_docs:
        log_error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æ¡£")
        return False

    log_info(f"åŠ è½½äº† {len(all_docs)} ä¸ªæ–‡æ¡£ï¼Œæ¥è‡ª {len(vector_files)} ä¸ªæ–‡ä»¶")

    # æ£€æµ‹é‡å¤å†…å®¹
    texts = [doc["text"] for doc in all_docs]
    duplicates = find_duplicates(texts)

    # è¿‡æ»¤æ‰é‡å¤å’Œä½è´¨é‡å†…å®¹
    filtered_docs = []
    for i, doc in enumerate(all_docs):
        # è·³è¿‡é‡å¤å†…å®¹
        if i in duplicates:
            continue

        # è·³è¿‡è¿‡çŸ­çš„å†…å®¹
        if len(doc["text"]) < 50:
            continue

        filtered_docs.append(doc)

    log_info(f"è¿‡æ»¤åå‰©ä½™ {len(filtered_docs)} ä¸ªæ–‡æ¡£")

    # åˆ é™¤åŸå§‹å‘é‡æ–‡ä»¶
    for file_path in vector_files:
        try:
            os.remove(file_path)
        except Exception as e:
            log_error(f"åˆ é™¤æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")

    # ä¿å­˜è¿‡æ»¤åçš„å‘é‡
    # æ¯ä¸ªæ–‡ä»¶æœ€å¤šä¿å­˜100ä¸ªæ–‡æ¡£
    chunk_size = 100
    for i in range(0, len(filtered_docs), chunk_size):
        chunk = filtered_docs[i:i+chunk_size]
        file_name = f"{uuid.uuid4()}.json"
        file_path = os.path.join(vectors_dir, file_name)

        try:
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(chunk, f, ensure_ascii=False, indent=2)
            log_info(f"ä¿å­˜äº† {len(chunk)} ä¸ªæ–‡æ¡£åˆ° {file_path}")
        except Exception as e:
            log_error(f"ä¿å­˜æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")

    # é‡å»ºç´¢å¼•
    log_info("å¼€å§‹é‡å»ºç´¢å¼•...")
    if build_faiss_index(kb_name) and build_pickle_index(kb_name):
        log_info("ç´¢å¼•é‡å»ºæˆåŠŸ")
        return True
    else:
        log_error("ç´¢å¼•é‡å»ºå¤±è´¥")
        return False

def find_duplicates(texts, similarity_threshold=0.9):
    """
    æŸ¥æ‰¾æ–‡æœ¬åˆ—è¡¨ä¸­çš„é‡å¤å†…å®¹

    å‚æ•°:
        texts: æ–‡æœ¬åˆ—è¡¨
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼è§†ä¸ºé‡å¤

    è¿”å›:
        é‡å¤æ–‡æœ¬çš„ç´¢å¼•åˆ—è¡¨
    """
    log_info(f"å¼€å§‹æŸ¥æ‰¾é‡å¤å†…å®¹ï¼Œå…± {len(texts)} ä¸ªæ–‡æœ¬å—")

    # å¦‚æœæ–‡æœ¬æ•°é‡å¤ªå°‘ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
    if len(texts) < 2:
        return []

    # ä½¿ç”¨é›†åˆå¿«é€ŸæŸ¥æ‰¾å®Œå…¨ç›¸åŒçš„æ–‡æœ¬
    seen_texts = {}
    exact_duplicates = []

    for i, text in enumerate(texts):
        # è§„èŒƒåŒ–æ–‡æœ¬ï¼ˆå»é™¤ç©ºç™½å­—ç¬¦ï¼‰
        normalized = re.sub(r'\s+', ' ', text).strip()

        # å¦‚æœæ–‡æœ¬å¤ªçŸ­ï¼Œè·³è¿‡
        if len(normalized) < 20:
            continue

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if normalized in seen_texts:
            exact_duplicates.append(i)
        else:
            seen_texts[normalized] = i

    log_info(f"æ‰¾åˆ° {len(exact_duplicates)} ä¸ªå®Œå…¨ç›¸åŒçš„æ–‡æœ¬å—")

    # å¦‚æœéœ€è¦æ›´ç²¾ç¡®çš„ç›¸ä¼¼åº¦æ£€æµ‹ï¼Œå¯ä»¥ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦
    # è¿™éœ€è¦è®¡ç®—æ¯ä¸ªæ–‡æœ¬çš„å‘é‡ï¼Œç„¶åæ¯”è¾ƒå‘é‡ç›¸ä¼¼åº¦
    # ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼Œä»…æ£€æµ‹å­—ç¬¦é‡å 

    fuzzy_duplicates = []
    if similarity_threshold < 1.0:
        # å¯¹äºå‰©ä½™çš„æ–‡æœ¬ï¼Œæ£€æŸ¥å†…å®¹é‡å 
        remaining_indices = [i for i in range(len(texts)) if i not in exact_duplicates]
        remaining_texts = [texts[i] for i in remaining_indices]

        for i in range(len(remaining_texts)):
            for j in range(i+1, len(remaining_texts)):
                # è®¡ç®—æ–‡æœ¬é‡å åº¦
                text1 = remaining_texts[i].lower()
                text2 = remaining_texts[j].lower()

                # å¦‚æœä¸¤ä¸ªæ–‡æœ¬é•¿åº¦ç›¸å·®å¤ªå¤§ï¼Œè·³è¿‡
                if len(text1) < 0.5 * len(text2) or len(text2) < 0.5 * len(text1):
                    continue

                # è®¡ç®—Jaccardç›¸ä¼¼åº¦
                words1 = set(text1.split())
                words2 = set(text2.split())

                if not words1 or not words2:
                    continue

                intersection = len(words1.intersection(words2))
                union = len(words1.union(words2))

                similarity = intersection / union if union > 0 else 0

                # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼Œæ ‡è®°ä¸ºé‡å¤
                if similarity > similarity_threshold:
                    fuzzy_duplicates.append(remaining_indices[j])

    log_info(f"æ‰¾åˆ° {len(fuzzy_duplicates)} ä¸ªç›¸ä¼¼æ–‡æœ¬å—")

    # åˆå¹¶ä¸¤ç§é‡å¤
    all_duplicates = list(set(exact_duplicates + fuzzy_duplicates))
    log_info(f"æ€»å…±æ‰¾åˆ° {len(all_duplicates)} ä¸ªé‡å¤æ–‡æœ¬å—")

    return all_duplicates

# æ·»åŠ åˆ é™¤çŸ¥è¯†åº“å‡½æ•°
def delete_knowledge_base(kb_name):
    kb_file = "knowledge_bases.json"
    try:
        if os.path.exists(kb_file) and os.path.getsize(kb_file) > 0:
            with open(kb_file, "r", encoding="utf-8") as f:
                knowledge_bases = json.load(f)
        else:
            knowledge_bases = []

        # ä»çŸ¥è¯†åº“åˆ—è¡¨ä¸­ç§»é™¤è¯¥çŸ¥è¯†åº“
        knowledge_bases = [kb for kb in knowledge_bases if kb["name"] != kb_name]

        # ä¿å­˜æ›´æ–°åçš„çŸ¥è¯†åº“åˆ—è¡¨
        with open(kb_file, "w", encoding="utf-8") as f:
            json.dump(knowledge_bases, f, ensure_ascii=False, indent=2)

        # åˆ é™¤çŸ¥è¯†åº“ç›®å½•
        kb_dir = os.path.join("knowledge_bases", kb_name)
        if os.path.exists(kb_dir):
            shutil.rmtree(kb_dir)

        log_info(f"çŸ¥è¯†åº“ '{kb_name}' å·²åˆ é™¤")
        return True
    except Exception as e:
        log_error(f"åˆ é™¤çŸ¥è¯†åº“ '{kb_name}' æ—¶å‡ºé”™: {e}")
        return False


def documents_page():
    st.title("ğŸ“„ çŸ¥è¯†åº“ç®¡ç†")
    st.write("åˆ›å»ºå’Œç®¡ç†ä¼ä¸šçŸ¥è¯†åº“ï¼Œä¸Šä¼ å’Œç»„ç»‡æ–‡æ¡£")

    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    if not os.path.exists("uploads"):
        os.makedirs("uploads")
    if not os.path.exists("knowledge_bases"):
        os.makedirs("knowledge_bases")

    # åŠ è½½çŸ¥è¯†åº“åˆ—è¡¨
    kb_file = "knowledge_bases.json"
    try:
        if os.path.exists(kb_file) and os.path.getsize(kb_file) > 0:
            with open(kb_file, "r", encoding="utf-8") as f:
                knowledge_bases = json.load(f)
        else:
            # æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œåˆ›å»ºæ–°çš„ç©ºåˆ—è¡¨
            knowledge_bases = []
            with open(kb_file, "w", encoding="utf-8") as f:
                json.dump(knowledge_bases, f, ensure_ascii=False, indent=2)
    except json.JSONDecodeError:
        # JSONè§£æé”™è¯¯ï¼Œæ–‡ä»¶å¯èƒ½æŸå
        log_error(f"çŸ¥è¯†åº“æ–‡ä»¶ {kb_file} æ ¼å¼é”™è¯¯ï¼Œåˆ›å»ºæ–°æ–‡ä»¶")
        knowledge_bases = []
        with open(kb_file, "w", encoding="utf-8") as f:
            json.dump(knowledge_bases, f, ensure_ascii=False, indent=2)

    # ä¾§è¾¹æ é…ç½®
    st.sidebar.subheader("âš™ï¸ çŸ¥è¯†åº“è®¾ç½®")

    # # åˆ›å»ºæ–°çŸ¥è¯†åº“
    # with st.sidebar.expander("â• åˆ›å»ºæ–°çŸ¥è¯†åº“", expanded=False):
    #     new_kb_name = st.text_input("çŸ¥è¯†åº“åç§°", key="new_kb_name")
    #     new_kb_desc = st.text_area("çŸ¥è¯†åº“æè¿°", key="new_kb_desc", height=100)
    #
    #     if st.button("åˆ›å»ºçŸ¥è¯†åº“", key="create_kb"):
    #         if not new_kb_name:
    #             st.error("è¯·è¾“å…¥çŸ¥è¯†åº“åç§°")
    #         elif any(kb["name"] == new_kb_name for kb in knowledge_bases):
    #             st.error(f"çŸ¥è¯†åº“ '{new_kb_name}' å·²å­˜åœ¨")
    #         else:
    #             # åˆå§‹åŒ–çŸ¥è¯†åº“ç›®å½•ç»“æ„
    #             if initialize_knowledge_base(new_kb_name):
    #                 # æ·»åŠ åˆ°çŸ¥è¯†åº“åˆ—è¡¨
    #                 knowledge_bases.append({
    #                     "name": new_kb_name,
    #                     "description": new_kb_desc,
    #                     "created_by": st.session_state['user_name'],
    #                     "created_at": time.strftime("%Y-%m-%d %H:%M:%S")
    #                 })
    #
    #                 # ä¿å­˜çŸ¥è¯†åº“åˆ—è¡¨
    #                 with open(kb_file, "w", encoding="utf-8") as f:
    #                     json.dump(knowledge_bases, f, ensure_ascii=False, indent=2)
    #
    #                 st.success(f"çŸ¥è¯†åº“ '{new_kb_name}' åˆ›å»ºæˆåŠŸï¼")
    #
    #                 # ä½¿ç”¨rerun()é‡æ–°åŠ è½½é¡µé¢ï¼Œè€Œä¸æ˜¯ç›´æ¥ä¿®æ”¹session_state
    #                 time.sleep(1)
    #                 st.rerun()
    #             else:
    #                 st.error(f"åˆ›å»ºçŸ¥è¯†åº“ '{new_kb_name}' å¤±è´¥")

    # åˆ›å»ºæ–°çŸ¥è¯†åº“
    with st.sidebar.expander("â• åˆ›å»ºæ–°çŸ¥è¯†åº“", expanded=False):
        if st.session_state.get('user_role') == 'ç®¡ç†å‘˜':
            new_kb_name = st.text_input("çŸ¥è¯†åº“åç§°", key="new_kb_name")
            new_kb_desc = st.text_area("çŸ¥è¯†åº“æè¿°", key="new_kb_desc", height=100)

            if st.button("åˆ›å»ºçŸ¥è¯†åº“", key="create_kb"):
                if not new_kb_name:
                    st.error("è¯·è¾“å…¥çŸ¥è¯†åº“åç§°")
                elif any(kb["name"] == new_kb_name for kb in knowledge_bases):
                    st.error(f"çŸ¥è¯†åº“ '{new_kb_name}' å·²å­˜åœ¨")
                else:
                    # åˆå§‹åŒ–çŸ¥è¯†åº“ç›®å½•ç»“æ„
                    if initialize_knowledge_base(new_kb_name):
                        # æ·»åŠ åˆ°çŸ¥è¯†åº“åˆ—è¡¨
                        knowledge_bases.append({
                            "name": new_kb_name,
                            "description": new_kb_desc,
                            "created_by": st.session_state['user_name'],
                            "created_at": time.strftime("%Y-%m-%d %H:%M:%S")
                        })

                        # ä¿å­˜çŸ¥è¯†åº“åˆ—è¡¨
                        with open(kb_file, "w", encoding="utf-8") as f:
                            json.dump(knowledge_bases, f, ensure_ascii=False, indent=2)

                        st.success(f"çŸ¥è¯†åº“ '{new_kb_name}' åˆ›å»ºæˆåŠŸï¼")

                        # ä½¿ç”¨rerun()é‡æ–°åŠ è½½é¡µé¢ï¼Œè€Œä¸æ˜¯ç›´æ¥ä¿®æ”¹session_state
                        time.sleep(1)
                        st.rerun()
                    else:
                        st.error(f"åˆ›å»ºçŸ¥è¯†åº“ '{new_kb_name}' å¤±è´¥")
        else:
            st.write("åªæœ‰ç®¡ç†å‘˜å¯ä»¥åˆ›å»ºçŸ¥è¯†åº“ã€‚")

    # é€‰æ‹©çŸ¥è¯†åº“
    kb_names = [kb["name"] for kb in knowledge_bases]
    if not kb_names:
        st.sidebar.warning("è¯·å…ˆåˆ›å»ºçŸ¥è¯†åº“")
        return

    selected_kb = st.sidebar.selectbox("ğŸ“š é€‰æ‹©çŸ¥è¯†åº“", kb_names)


    # åˆ é™¤çŸ¥è¯†åº“æŒ‰é’®
    if st.sidebar.button(f"ğŸ—‘ï¸ åˆ é™¤çŸ¥è¯†åº“ '{selected_kb}'"):
        if delete_knowledge_base(selected_kb):
            st.success(f"çŸ¥è¯†åº“ '{selected_kb}' å·²åˆ é™¤")
            time.sleep(1)
            st.rerun()
        else:
            st.error(f"åˆ é™¤çŸ¥è¯†åº“ '{selected_kb}' å¤±è´¥")

    # è·å–æ‰€æœ‰éƒ¨é—¨åˆ—è¡¨
    all_departments = st.session_state['user_knowledge_access']

    # å½“å‰ç”¨æˆ·çš„éƒ¨é—¨
    user_department = st.session_state['user_department']

    # åˆ¤æ–­ç”¨æˆ·æ˜¯å¦ä¸ºç®¡ç†å‘˜
    user_role = st.session_state.get('user_role', '')
    is_admin = user_role == 'ç®¡ç†å‘˜'

    # åˆ›å»ºé€‰é¡¹å¡
    tab1, tab2, tab3 = st.tabs(["ä¸Šä¼ æ–‡æ¡£", "å¤„ç†æ–‡æ¡£", "ç®¡ç†æ–‡æ¡£"])

    # with tab1:
    #     st.subheader(f"ä¸Šä¼ æ–‡æ¡£åˆ° '{selected_kb}' çŸ¥è¯†åº“")
    #
    #     # è·å–ç”¨æˆ·å¯è®¿é—®çš„éƒ¨é—¨åˆ—è¡¨
    #     accessible_depts = st.session_state['user_knowledge_access']
    #
    #     # æ·»åŠ éƒ¨é—¨é€‰æ‹©
    #     selected_department = st.selectbox(
    #         "é€‰æ‹©æ–‡æ¡£æ‰€å±éƒ¨é—¨",
    #         accessible_depts,
    #         help="é€‰æ‹©æ–‡æ¡£æ‰€å±çš„éƒ¨é—¨ï¼Œè¿™å°†å†³å®šå“ªäº›ç”¨æˆ·å¯ä»¥è®¿é—®è¯¥æ–‡æ¡£"
    #     )
    #
    #     # ä¸Šä¼ æ–‡ä»¶
    #     uploaded_files = st.file_uploader("é€‰æ‹©è¦ä¸Šä¼ çš„æ–‡ä»¶", accept_multiple_files=True)
    #
    #     if uploaded_files:
    #         upload_button = st.button("ğŸ“¤ ä¸Šä¼ æ–‡ä»¶", use_container_width=True)
    #
    #         if upload_button:
    #             # åˆ›å»ºç›®å½•
    #             kb_dir = os.path.join("knowledge_bases", selected_kb)
    #             docs_dir = os.path.join(kb_dir, "documents")
    #             meta_dir = os.path.join(kb_dir, "metadata")
    #
    #             os.makedirs(docs_dir, exist_ok=True)
    #             os.makedirs(meta_dir, exist_ok=True)
    #
    #             # æ£€æŸ¥æˆ–åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶
    #             meta_file = os.path.join(meta_dir, "documents.csv")
    #             if os.path.exists(meta_file):
    #                 df = pd.read_csv(meta_file)
    #             else:
    #                 df = pd.DataFrame(columns=[
    #                     "title", "file_path", "file_type", "department",
    #                     "uploaded_by", "upload_time", "processed"
    #                 ])
    #
    #             # å¤„ç†æ¯ä¸ªä¸Šä¼ çš„æ–‡ä»¶
    #             for uploaded_file in uploaded_files:
    #                 # ä¿å­˜æ–‡ä»¶ - ä¿®å¤å‚æ•°é—®é¢˜
    #                 file_name = uploaded_file.name
    #                 file_path = os.path.join(docs_dir, file_name)
    #                 save_uploaded_file(uploaded_file, docs_dir, file_name)
    #
    #                 # è·å–æ–‡ä»¶ç±»å‹
    #                 file_extension = os.path.splitext(file_name)[1].lower().replace(".", "")
    #
    #                 # åˆ›å»ºå…ƒæ•°æ®
    #                 metadata = {
    #                     "title": file_name,
    #                     "file_path": file_path,
    #                     "file_type": file_extension,
    #                     "department": selected_department,  # ä½¿ç”¨é€‰æ‹©çš„éƒ¨é—¨
    #                     "uploaded_by": st.session_state.get("username", "æœªçŸ¥ç”¨æˆ·"),
    #                     "upload_time": time.strftime("%Y-%m-%d %H:%M:%S"),
    #                     "processed": False
    #                 }
    #
    #                 # æ·»åŠ åˆ°å…ƒæ•°æ®
    #                 df = pd.concat([df, pd.DataFrame([metadata])], ignore_index=True)
    #
    #                 st.info(f"å·²ä¸Šä¼ : {file_name}")
    #
    #             # ä¿å­˜å…ƒæ•°æ®
    #             df.to_csv(meta_file, index=False)
    #
    #             # ä¸Šä¼ æˆåŠŸåï¼Œè¯¢é—®æ˜¯å¦ç«‹å³å¤„ç†å‘é‡
    #             if st.success(f"æˆåŠŸä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶"):
    #                 if st.button("ç«‹å³å¤„ç†æ–‡æ¡£å‘é‡", key="process_vectors"):
    #                     with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£å‘é‡..."):
    #                         try:
    #                             # è¿æ¥åˆ°xinferenceæœåŠ¡
    #                             client = RESTfulClient("http://localhost:9997")
    #
    #                             # å¤„ç†æœ€æ–°ä¸Šä¼ çš„æ–‡ä»¶
    #                             for i, row in df.tail(len(uploaded_files)).iterrows():
    #                                 if process_document_vectors(selected_kb, row, client):
    #                                     # æ›´æ–°å¤„ç†çŠ¶æ€
    #                                     df.at[i, "processed"] = True
    #
    #                             # ä¿å­˜æ›´æ–°åçš„å…ƒæ•°æ®
    #                             df.to_csv(meta_file, index=False)
    #
    #                             st.success("æ–‡æ¡£å‘é‡å¤„ç†å®Œæˆï¼")
    #                         except Exception as e:
    #                             st.error(f"å¤„ç†æ–‡æ¡£å‘é‡æ—¶å‡ºé”™: {e}")

    with tab1:
        user_role = st.session_state.get('user_role', 'æ™®é€šç”¨æˆ·')
        if user_role != 'æ™®é€šç”¨æˆ·':
            st.subheader(f"ä¸Šä¼ æ–‡æ¡£åˆ° '{selected_kb}' çŸ¥è¯†åº“")

            # è·å–ç”¨æˆ·å¯è®¿é—®çš„éƒ¨é—¨åˆ—è¡¨
            accessible_depts = st.session_state['user_knowledge_access']

            # æ·»åŠ éƒ¨é—¨é€‰æ‹©
            selected_department = st.selectbox(
                "é€‰æ‹©æ–‡æ¡£æ‰€å±éƒ¨é—¨",
                accessible_depts,
                help="é€‰æ‹©æ–‡æ¡£æ‰€å±çš„éƒ¨é—¨ï¼Œè¿™å°†å†³å®šå“ªäº›ç”¨æˆ·å¯ä»¥è®¿é—®è¯¥æ–‡æ¡£"
            )

            # ä¸Šä¼ æ–‡ä»¶
            uploaded_files = st.file_uploader("é€‰æ‹©è¦ä¸Šä¼ çš„æ–‡ä»¶", accept_multiple_files=True)

            if uploaded_files:
                upload_button = st.button("ğŸ“¤ ä¸Šä¼ æ–‡ä»¶", use_container_width=True)

                # if upload_button:
                #     # åˆ›å»ºç›®å½•
                #     kb_dir = os.path.join("knowledge_bases", selected_kb)
                #     docs_dir = os.path.join(kb_dir, "documents")
                #     meta_dir = os.path.join(kb_dir, "metadata")
                #
                #     os.makedirs(docs_dir, exist_ok=True)
                #     os.makedirs(meta_dir, exist_ok=True)
                #
                #     # æ£€æŸ¥æˆ–åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶
                #     meta_file = os.path.join(meta_dir, "documents.csv")
                #     if os.path.exists(meta_file):
                #         df = pd.read_csv(meta_file)
                #     else:
                #         df = pd.DataFrame(columns=[
                #             "title", "file_path", "file_type", "department",
                #             "uploaded_by", "upload_time", "processed"
                #         ])
                #
                #     # å¤„ç†æ¯ä¸ªä¸Šä¼ çš„æ–‡ä»¶
                #     for uploaded_file in uploaded_files:
                #         # ä¿å­˜æ–‡ä»¶ - ä¿®å¤å‚æ•°é—®é¢˜
                #         file_name = uploaded_file.name
                #         file_path = os.path.join(docs_dir, file_name)
                #         save_uploaded_file(uploaded_file, docs_dir, file_name)
                #
                #         # è·å–æ–‡ä»¶ç±»å‹
                #         file_extension = os.path.splitext(file_name)[1].lower().replace(".", "")
                #
                #         # åˆ›å»ºå…ƒæ•°æ®
                #         metadata = {
                #             "title": file_name,
                #             "file_path": file_path,
                #             "file_type": file_extension,
                #             "department": selected_department,  # ä½¿ç”¨é€‰æ‹©çš„éƒ¨é—¨
                #             "uploaded_by": st.session_state.get("username", "æœªçŸ¥ç”¨æˆ·"),
                #             "upload_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                #             "processed": False
                #         }
                #
                #         # æ·»åŠ åˆ°å…ƒæ•°æ®
                #         df = pd.concat([df, pd.DataFrame([metadata])], ignore_index=True)
                #
                #         st.info(f"å·²ä¸Šä¼ : {file_name}")
                #
                #     # ä¿å­˜å…ƒæ•°æ®
                #     df.to_csv(meta_file, index=False)
                #
                #     # ä¸Šä¼ æˆåŠŸåï¼Œè¯¢é—®æ˜¯å¦ç«‹å³å¤„ç†å‘é‡
                #     if st.success(f"æˆåŠŸä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶"):
                #         if st.button("ç«‹å³å¤„ç†æ–‡æ¡£å‘é‡", key="process_vectors"):
                #             with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£å‘é‡..."):
                #                 try:
                #                     # è¿æ¥åˆ°xinferenceæœåŠ¡
                #                     client = RESTfulClient("http://localhost:9997")
                #
                #                     # å¤„ç†æœ€æ–°ä¸Šä¼ çš„æ–‡ä»¶
                #                     for i, row in df.tail(len(uploaded_files)).iterrows():
                #                         if process_document_vectors(selected_kb, row, client):
                #                             # æ›´æ–°å¤„ç†çŠ¶æ€
                #                             df.at[i, "processed"] = True
                #
                #                     # ä¿å­˜æ›´æ–°åçš„å…ƒæ•°æ®
                #                     df.to_csv(meta_file, index=False)
                #
                #                     st.success("æ–‡æ¡£å‘é‡å¤„ç†å®Œæˆï¼")
                #                 except Exception as e:
                #                     st.error(f"å¤„ç†æ–‡æ¡£å‘é‡æ—¶å‡ºé”™: {e}")
                if upload_button:
                    # æ£€æŸ¥ç®¡ç†éƒ¨æƒé™
                    if selected_department == 'ç®¡ç†éƒ¨' and not is_admin:
                        st.error("åªæœ‰ç®¡ç†å‘˜å¯ä»¥ä¸Šä¼ ç®¡ç†éƒ¨çš„æ–‡æ¡£")
                        return

                    # åˆ›å»ºç›®å½•
                    kb_dir = os.path.join("knowledge_bases", selected_kb)
                    docs_dir = os.path.join(kb_dir, "documents")
                    meta_dir = os.path.join(kb_dir, "metadata")

                    os.makedirs(docs_dir, exist_ok=True)
                    os.makedirs(meta_dir, exist_ok=True)

                    # æ£€æŸ¥æˆ–åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶
                    meta_file = os.path.join(meta_dir, "documents.csv")
                    if os.path.exists(meta_file):
                        df = pd.read_csv(meta_file)
                    else:
                        df = pd.DataFrame(columns=[
                            "title", "file_path", "file_type", "department",
                            "uploaded_by", "upload_time", "processed"
                        ])

                    # å¤„ç†æ¯ä¸ªä¸Šä¼ çš„æ–‡ä»¶
                    for uploaded_file in uploaded_files:
                        # ä¿å­˜æ–‡ä»¶ - ä¿®å¤å‚æ•°é—®é¢˜
                        file_name = uploaded_file.name
                        file_path = os.path.join(docs_dir, file_name)
                        save_uploaded_file(uploaded_file, docs_dir, file_name)

                        # è·å–æ–‡ä»¶ç±»å‹
                        file_extension = os.path.splitext(file_name)[1].lower().replace(".", "")

                        # åˆ›å»ºå…ƒæ•°æ®
                        metadata = {
                            "title": file_name,
                            "file_path": file_path,
                            "file_type": file_extension,
                            "department": selected_department,  # ä½¿ç”¨é€‰æ‹©çš„éƒ¨é—¨
                            "uploaded_by": st.session_state.get("username", "æœªçŸ¥ç”¨æˆ·"),
                            "upload_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                            "processed": False
                        }

                        # æ·»åŠ åˆ°å…ƒæ•°æ®
                        df = pd.concat([df, pd.DataFrame([metadata])], ignore_index=True)

                        st.info(f"å·²ä¸Šä¼ : {file_name}")

                    # ä¿å­˜å…ƒæ•°æ®
                    df.to_csv(meta_file, index=False)

                    # ä¸Šä¼ æˆåŠŸåï¼Œè¯¢é—®æ˜¯å¦ç«‹å³å¤„ç†å‘é‡
                    if st.success(f"æˆåŠŸä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶"):
                        if st.button("ç«‹å³å¤„ç†æ–‡æ¡£å‘é‡", key="process_vectors"):
                            with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£å‘é‡..."):
                                try:
                                    # è¿æ¥åˆ°xinferenceæœåŠ¡
                                    client = RESTfulClient("http://localhost:9997")

                                    # å¤„ç†æœ€æ–°ä¸Šä¼ çš„æ–‡ä»¶
                                    for i, row in df.tail(len(uploaded_files)).iterrows():
                                        if process_document_vectors(selected_kb, row, client):
                                            # æ›´æ–°å¤„ç†çŠ¶æ€
                                            df.at[i, "processed"] = True

                                    # ä¿å­˜æ›´æ–°åçš„å…ƒæ•°æ®
                                    df.to_csv(meta_file, index=False)

                                    st.success("æ–‡æ¡£å‘é‡å¤„ç†å®Œæˆï¼")
                                except Exception as e:
                                    st.error(f"å¤„ç†æ–‡æ¡£å‘é‡æ—¶å‡ºé”™: {e}")
        else:
            st.write("æ™®é€šç”¨æˆ·æ²¡æœ‰ä¸Šä¼ æ–‡æ¡£çš„æƒé™ã€‚")

    with tab2:
        st.subheader(f"å¤„ç† '{selected_kb}' çŸ¥è¯†åº“æ–‡æ¡£")

        # æ£€æŸ¥å…ƒæ•°æ®æ–‡ä»¶
        meta_file = os.path.join("knowledge_bases", selected_kb, "metadata", "documents.csv")
        if not os.path.exists(meta_file):
            st.warning("æ²¡æœ‰å¯å¤„ç†çš„æ–‡æ¡£")
        else:
            df = pd.read_csv(meta_file)
            unprocessed = df[df["processed"] == False]

            # æ˜¾ç¤ºå¤„ç†çŠ¶æ€
            col1, col2 = st.columns(2)
            with col1:
                st.metric("æ€»æ–‡æ¡£æ•°", len(df))
            with col2:
                st.metric("æœªå¤„ç†æ–‡æ¡£", len(unprocessed))

            # å¤„ç†æŒ‰é’®
            if len(unprocessed) > 0:
                process_button = st.button("ğŸ”„ å¤„ç†å¹¶å»ºç«‹å‘é‡ç´¢å¼•", use_container_width=True)

                if process_button:
                    with st.spinner(f"æ­£åœ¨å¤„ç† {len(unprocessed)} ä¸ªæ–‡æ¡£..."):
                        if not XINFERENCE_AVAILABLE:
                            st.warning("æ¨¡æ‹Ÿå¤„ç†æ¨¡å¼ï¼šxinferenceæ¨¡å—æœªå®‰è£…")

                        try:
                            # è¿æ¥åˆ°xinferenceæœåŠ¡
                            client = RESTfulClient("http://localhost:9997")

                            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
                            if XINFERENCE_AVAILABLE:
                                try:
                                    models = client.list_models()

                                    if isinstance(models, str):
                                        try:
                                            models = json.loads(models)
                                        except:
                                            pass

                                    if isinstance(models, list):
                                        model_exists = False
                                        for model in models:
                                            if isinstance(model, dict) and model.get("model_name") == "bge-large-zh-v1.5":
                                                model_exists = True
                                                break
                                    else:
                                        model_exists = "bge-large-zh-v1.5" in str(models)

                                    if not model_exists:
                                        # åŠ è½½æ¨¡å‹
                                        result = client.launch_model(
                                            model_name="bge-large-zh-v1.5",
                                            model_type="embedding"
                                        )
                                except Exception as e:
                                    st.error(f"æ£€æŸ¥æ¨¡å‹æ—¶å‡ºé”™: {e}")

                            # å¤„ç†æ¯ä¸ªæœªå¤„ç†çš„æ–‡æ¡£
                            success_count = 0
                            progress_bar = st.progress(0)
                            total_docs = len(unprocessed)

                            for i, (idx, row) in enumerate(unprocessed.iterrows()):
                                progress_bar.progress((i + 1) / total_docs)
                                success = process_document_vectors(selected_kb, row, client)

                                if success:
                                    df.at[idx, "processed"] = True
                                    success_count += 1
                                    st.info(f"å·²å¤„ç†: {i+1}/{total_docs} - {row['title']}")
                                else:
                                    st.error(f"å¤„ç†å¤±è´¥: {row['title']}")

                            df.to_csv(meta_file, index=False)

                            if success_count > 0:
                                with st.spinner("æ­£åœ¨ä¼˜åŒ–çŸ¥è¯†åº“..."):
                                    # è‡ªåŠ¨è¿‡æ»¤é‡å¤å†…å®¹
                                    filter_knowledge_base(selected_kb)

                                    # æ„å»ºç´¢å¼•
                                    if FAISS_AVAILABLE:
                                        try:
                                            if build_faiss_index(selected_kb):
                                                st.success("FAISSç´¢å¼•æ„å»ºæˆåŠŸï¼")
                                            else:
                                                st.warning("FAISSç´¢å¼•æ„å»ºå¤±è´¥")
                                        except Exception as e:
                                            st.error(f"æ„å»ºFAISSç´¢å¼•æ—¶å‡ºé”™: {e}")

                                    try:
                                        if build_pickle_index(selected_kb):
                                            st.success("Pickleç´¢å¼•æ„å»ºæˆåŠŸï¼")
                                        else:
                                            st.warning("Pickleç´¢å¼•æ„å»ºå¤±è´¥")
                                    except Exception as e:
                                        st.error(f"æ„å»ºPickleç´¢å¼•æ—¶å‡ºé”™: {e}")

                            st.success(f"å¤„ç†å®Œæˆï¼æˆåŠŸ: {success_count}, å¤±è´¥: {len(unprocessed) - success_count}")
                        except Exception as e:
                            if XINFERENCE_AVAILABLE:
                                st.error(f"æ— æ³•è¿æ¥åˆ°xinferenceæœåŠ¡: {e}")
                            else:
                                st.error(f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {e}")
            else:
                st.success("æ‰€æœ‰æ–‡æ¡£å·²å¤„ç†å®Œæˆ")

            if len(df) > 0:
                st.subheader("æ–‡æ¡£å¤„ç†çŠ¶æ€")
                st.dataframe(
                    df[["title", "department", "processed", "upload_time"]],
                    use_container_width=True,
                    column_config={
                        "title": "æ–‡ä»¶å",
                        "department": "æ‰€å±éƒ¨é—¨",
                        "processed": "å·²å¤„ç†",
                        "upload_time": "ä¸Šä¼ æ—¶é—´"
                    }
                )

    # with tab3:
    #     st.subheader(f"'{selected_kb}' çŸ¥è¯†åº“æ–‡æ¡£ç®¡ç†")
    #
    #     meta_file = os.path.join("knowledge_bases", selected_kb, "metadata", "documents.csv")
    #     if os.path.exists(meta_file):
    #         df = pd.read_csv(meta_file)
    #
    #         accessible_depts = st.session_state['user_knowledge_access']
    #         filtered_df = df[df["department"].isin(accessible_depts)]
    #
    #         if len(filtered_df) > 0:
    #             st.dataframe(
    #                 filtered_df[["title", "department", "uploaded_by", "upload_time", "processed"]],
    #                 use_container_width=True,
    #                 column_config={
    #                     "title": "æ–‡ä»¶å",
    #                     "department": "æ‰€å±éƒ¨é—¨",
    #                     "uploaded_by": "ä¸Šä¼ è€…",
    #                     "upload_time": "ä¸Šä¼ æ—¶é—´",
    #                     "processed": "å·²å¤„ç†"
    #                 }
    #             )
    #
    #             selected_doc = st.selectbox("é€‰æ‹©æ–‡æ¡£è¿›è¡Œæ“ä½œ", filtered_df["title"].tolist())
    #
    #             if selected_doc:
    #                 doc_data = filtered_df[filtered_df["title"] == selected_doc].iloc[0]
    #
    #                 st.write("### æ–‡æ¡£è¯¦æƒ…")
    #                 st.write(f"**æ–‡ä»¶å:** {doc_data['title']}")
    #                 st.write(f"**éƒ¨é—¨:** {doc_data['department']}")
    #                 st.write(f"**ä¸Šä¼ è€…:** {doc_data['uploaded_by']}")
    #                 st.write(f"**ä¸Šä¼ æ—¶é—´:** {doc_data['upload_time']}")
    #                 st.write(f"**æ–‡ä»¶ç±»å‹:** {doc_data['file_type']}")
    #                 st.write(f"**å¤„ç†çŠ¶æ€:** {'å·²å¤„ç†' if doc_data['processed'] else 'æœªå¤„ç†'}")
    #
    #                 col1, col2 = st.columns(2)
    #
    #                 with col1:
    #                     if st.button("ğŸ—‘ï¸ åˆ é™¤æ–‡æ¡£", use_container_width=True):
    #                         try:
    #                             os.remove(doc_data["file_path"])
    #                         except:
    #                             st.warning("æ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— æ³•åˆ é™¤")
    #
    #                         df = df[df["title"] != selected_doc]
    #                         df.to_csv(meta_file, index=False)
    #
    #                         st.success(f"æ–‡æ¡£ '{selected_doc}' å·²åˆ é™¤")
    #                         time.sleep(1)
    #                         st.rerun()
    #
    #                 with col2:
    #                     if not doc_data["processed"]:
    #                         if st.button("ğŸ”„ å¤„ç†æ–‡æ¡£", use_container_width=True):
    #                             try:
    #                                 client = RESTfulClient("http://localhost:9997")
    #                                 success = process_document_vectors(selected_kb, doc_data, client)
    #
    #                                 if success:
    #                                     idx = df[df["title"] == selected_doc].index[0]
    #                                     df.at[idx, "processed"] = True
    #                                     df.to_csv(meta_file, index=False)
    #
    #                                     st.info("æ­£åœ¨æ›´æ–°å‘é‡ç´¢å¼•...")
    #
    #                                     if FAISS_AVAILABLE:
    #                                         try:
    #                                             if build_faiss_index(selected_kb):
    #                                                 st.success("FAISSç´¢å¼•æ›´æ–°æˆåŠŸï¼")
    #                                             else:
    #                                                 st.warning("FAISSç´¢å¼•æ›´æ–°å¤±è´¥")
    #                                         except Exception as e:
    #                                             st.error(f"æ›´æ–°FAISSç´¢å¼•æ—¶å‡ºé”™: {e}")
    #
    #                                     try:
    #                                         if build_pickle_index(selected_kb):
    #                                             st.success("Pickleç´¢å¼•æ›´æ–°æˆåŠŸï¼")
    #                                         else:
    #                                             st.warning("Pickleç´¢å¼•æ›´æ–°å¤±è´¥")
    #                                     except Exception as e:
    #                                         st.error(f"æ›´æ–°Pickleç´¢å¼•æ—¶å‡ºé”™: {e}")
    #
    #                                     st.success(f"æ–‡æ¡£ '{selected_doc}' å·²å¤„ç†")
    #                                 else:
    #                                     st.error(f"å¤„ç†æ–‡æ¡£ '{selected_doc}' å¤±è´¥")
    #                             except Exception as e:
    #                                 st.error(f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {e}")
    #
    #                             time.sleep(1)
    #                             st.rerun()
    #         else:
    #             st.info("æ²¡æœ‰å¯è®¿é—®çš„æ–‡æ¡£")
    #     else:
    #         st.info("çŸ¥è¯†åº“ä¸­è¿˜æ²¡æœ‰æ–‡æ¡£")

    with tab3:
        st.subheader(f"'{selected_kb}' çŸ¥è¯†åº“æ–‡æ¡£ç®¡ç†")

        meta_file = os.path.join("knowledge_bases", selected_kb, "metadata", "documents.csv")
        if os.path.exists(meta_file):
            df = pd.read_csv(meta_file)

            accessible_depts = st.session_state['user_knowledge_access']
            filtered_df = df[df["department"].isin(accessible_depts)]

            if len(filtered_df) > 0:
                st.dataframe(
                    filtered_df[["title", "department", "uploaded_by", "upload_time", "processed"]],
                    use_container_width=True,
                    column_config={
                        "title": "æ–‡ä»¶å",
                        "department": "æ‰€å±éƒ¨é—¨",
                        "uploaded_by": "ä¸Šä¼ è€…",
                        "upload_time": "ä¸Šä¼ æ—¶é—´",
                        "processed": "å·²å¤„ç†"
                    }
                )

                selected_doc = st.selectbox("é€‰æ‹©æ–‡æ¡£è¿›è¡Œæ“ä½œ", filtered_df["title"].tolist())

                if selected_doc:
                    doc_data = filtered_df[filtered_df["title"] == selected_doc].iloc[0]

                    st.write("### æ–‡æ¡£è¯¦æƒ…")
                    st.write(f"**æ–‡ä»¶å:** {doc_data['title']}")
                    st.write(f"**éƒ¨é—¨:** {doc_data['department']}")
                    st.write(f"**ä¸Šä¼ è€…:** {doc_data['uploaded_by']}")
                    st.write(f"**ä¸Šä¼ æ—¶é—´:** {doc_data['upload_time']}")
                    st.write(f"**æ–‡ä»¶ç±»å‹:** {doc_data['file_type']}")
                    st.write(f"**å¤„ç†çŠ¶æ€:** {'å·²å¤„ç†' if doc_data['processed'] else 'æœªå¤„ç†'}")

                    col1, col2 = st.columns(2)

                    with col1:
                        # æ£€æŸ¥ç®¡ç†éƒ¨æƒé™
                        if doc_data['department'] == 'ç®¡ç†éƒ¨' and not is_admin:
                            st.write("åªæœ‰ç®¡ç†å‘˜å¯ä»¥åˆ é™¤ç®¡ç†éƒ¨çš„æ–‡æ¡£")
                        else:
                            if st.button("ğŸ—‘ï¸ åˆ é™¤æ–‡æ¡£", use_container_width=True):
                                try:
                                    os.remove(doc_data["file_path"])
                                except:
                                    st.warning("æ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— æ³•åˆ é™¤")

                                df = df[df["title"] != selected_doc]
                                df.to_csv(meta_file, index=False)

                                st.success(f"æ–‡æ¡£ '{selected_doc}' å·²åˆ é™¤")
                                time.sleep(1)
                                st.rerun()

                    with col2:
                        if not doc_data["processed"]:
                            if st.button("ğŸ”„ å¤„ç†æ–‡æ¡£", use_container_width=True):
                                try:
                                    client = RESTfulClient("http://localhost:9997")
                                    success = process_document_vectors(selected_kb, doc_data, client)

                                    if success:
                                        idx = df[df["title"] == selected_doc].index[0]
                                        df.at[idx, "processed"] = True
                                        df.to_csv(meta_file, index=False)

                                        st.info("æ­£åœ¨æ›´æ–°å‘é‡ç´¢å¼•...")

                                        if FAISS_AVAILABLE:
                                            try:
                                                if build_faiss_index(selected_kb):
                                                    st.success("FAISSç´¢å¼•æ›´æ–°æˆåŠŸï¼")
                                                else:
                                                    st.warning("FAISSç´¢å¼•æ›´æ–°å¤±è´¥")
                                            except Exception as e:
                                                st.error(f"æ›´æ–°FAISSç´¢å¼•æ—¶å‡ºé”™: {e}")

                                        try:
                                            if build_pickle_index(selected_kb):
                                                st.success("Pickleç´¢å¼•æ›´æ–°æˆåŠŸï¼")
                                            else:
                                                st.warning("Pickleç´¢å¼•æ›´æ–°å¤±è´¥")
                                        except Exception as e:
                                            st.error(f"æ›´æ–°Pickleç´¢å¼•æ—¶å‡ºé”™: {e}")

                                        st.success(f"æ–‡æ¡£ '{selected_doc}' å·²å¤„ç†")
                                    else:
                                        st.error(f"å¤„ç†æ–‡æ¡£ '{selected_doc}' å¤±è´¥")
                                except Exception as e:
                                    st.error(f"å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {e}")

                                time.sleep(1)
                                st.rerun()
            else:
                st.info("æ²¡æœ‰å¯è®¿é—®çš„æ–‡æ¡£")
        else:
            st.info("çŸ¥è¯†åº“ä¸­è¿˜æ²¡æœ‰æ–‡æ¡£")